description: long-context-qwen2.5-vl-3b-instruct-vtvt-2-zs on Nvidia

target:
  service: sing
  #name: whitney10 
  # name: palisades15
  name: msroctobasicvc
  #name: msroctobasicvc@gcr-singularity-octo
  workspace_name: lightyear

environment:
  # registry: reubencr.azurecr.io
  # image: amlt-sing/acpt-rocm6.1_ubuntu20.04_py3.9_pytorch2.1.2
  image: amlt-sing/acpt-torch2.5.0-py3.10-cuda12.4-ubuntu22.04
  #image: rocm/vllm0.6.3-rocm6.2.3-pytorch2.6-fa2.7-singularity-ib:2.0

  setup:
    - source activate
    - conda create -n long python=3.11
    - conda activate long

storage:
  # vlpdatasets:
  #   storage_account_name: vlpdatasets
  #   container_name: data  
  # model:
  #   storage_account_name: projects4jw
  #   container_name: model
  # output:
  #   storage_account_name: reubenprojects
  #   container_name: model
  data:
    storage_account_name: ruisun
    container_name: data
  model:
    storage_account_name: ruisun
    container_name: model
  # output:
  #   storage_account_name: ruisun
  #   container_name: model

code:
  local_dir: ./

# search:
jobs:
# job template
  #name: premium-amd-pretrain-col33-step56000-rope-template-1.6M-pooling-{mm_use_spatial_pooling}-seqlen-{max_seq_len}-ft-{tune_vision_tokenizer}-frames-{max_num_frames}-finetune-{max_num_crops}-anyres{img_anyres_strategy}-{tune_vision_tokenizer}-{finetune_eps}-{sft_data}_ise{mm_use_image_start_end}_tse{mm_use_trace_start_end}_tsd{mm_use_trace_speed}_rtpts{remove_static_trace_pts}_qsz{spatial_quant_size}
  #name: premium-amd-pretraining_col34-step42000-rope-template-1.6M-pooling-{mm_use_spatial_pooling}-seqlen-{max_seq_len}-ft-{tune_vision_tokenizer}-frames-{max_num_frames}-finetune-{max_num_crops}-anyres{img_anyres_strategy}-{tune_vision_tokenizer}-{finetune_eps}-{sft_data}_ise{mm_use_image_start_end}_tse{mm_use_trace_start_end}_tsd{mm_use_trace_speed}_rtpts{remove_static_trace_pts}_qsz{spatial_quant_size}
  #name: premium-amd-ablate-row3-col7-step14000-rope-template-1.6M-pooling-{mm_use_spatial_pooling}-seqlen-{max_seq_len}-ft-{tune_vision_tokenizer}-frames-{max_num_frames}-finetune-{max_num_crops}-anyres{img_anyres_strategy}-{tune_vision_tokenizer}-{finetune_eps}-{sft_data}_ise{mm_use_image_start_end}_tse{mm_use_trace_start_end}_tsd{mm_use_trace_speed}_rtpts{remove_static_trace_pts}_qsz{spatial_quant_size}
  #name: run3-premium-amd-ablate-row3-col7-step14000-rope-template-1.6M-pooling-{mm_use_spatial_pooling}-seqlen-{max_seq_len}-ft-{tune_vision_tokenizer}-frames-{max_num_frames}-finetune-{max_num_crops}-anyres{img_anyres_strategy}-{tune_vision_tokenizer}-{finetune_eps}-{sft_data}_ise{mm_use_image_start_end}_tse{mm_use_trace_start_end}_tsd{mm_use_trace_speed}_rtpts{remove_static_trace_pts}_qsz{spatial_quant_size}
  #name: premium-amd-scaling-row1-col4-step42000-rope-template-1.6M-pooling-{mm_use_spatial_pooling}-seqlen-{max_seq_len}-ft-{tune_vision_tokenizer}-frames-{max_num_frames}-finetune-{max_num_crops}-anyres{img_anyres_strategy}-{tune_vision_tokenizer}-{finetune_eps}-{sft_data}_ise{mm_use_image_start_end}_tse{mm_use_trace_start_end}_tsd{mm_use_trace_speed}_rtpts{remove_static_trace_pts}_qsz{spatial_quant_size}
  - name: long-context-qwen2.5-vl-3b-instruct-vtvt-2-zs
    sku: 80G8-A100 # 80G1-A100 1xG8-A100 80G8-H100-IB-NvLink
    #sku: 8xG8-MI300X-IB-xGMI
    #sku: 8x192G8-MI300X-IB-xGMI@westcentralus
    mpi: False
    process_count_per_node: 1
    #sla_tier: Basic
    #sla_tier: Standard
    identity: managed
    command:
      - source activate
      - conda activate long
      - pip install -r requirements3n.txt
      - conda install -c conda-forge glib
      # sft
      # - deepspeed --include localhost:0,1,2,3 --master_port 5678 train.py \
      #   --wandb_key=$$WANDB_KEY \
      #   --model_id="showlab/ShowUI-2B" \
      #   --version='showlab/ShowUI-2B' \
      #   --dataset_dir=$$_DATA_DIR \
      #   --log_base_dir=$$_SAVE_DIR \
      #   --epochs=50 \
      #   --steps_per_epoch=100 \
      #   --batch_size=1 \
      #   --grad_accumulation_steps=2 \
      #   --model_max_length=8192 \
      #   --exp_id="showlab/ShowUI-2B" \
      #   --train_ratio="1" \
      #   --train_dataset="showui"  \
      #   --train_json="hf_train"   \
      #   --val_dataset="mind2web"  \
      #   --precision="bf16" \
      #   --attn_imple="sdpa" \
      #   --workers=4 \
      #   --lora_r=0 \
      #   --lora_alpha=64  \
      #   --min_visual_tokens=256  \
      #   --max_visual_tokens=1344 \
      #   --num_turn=100 \
      #   --crop_min=0.5 \
      #   --crop_max=1.5 \
      #   --record_sample \
      #   --lr=0.0001 \
      #   --uniform_prompt  \
      #   --ds_zero="zero2" \
      #   --gradient_checkpointing \
      #   --eval_only
      # zs
      # - deepspeed --hostfile=hostfile --master_port 5678 train.py
      # - python -m torch.distributed.run --nproc_per_node 8 --nnodes 1 --node_rank 0 --master_port 9600  --master_addr node-0 train.py
      #   --wandb_key=$$WANDB_API_KEY
      #   --model_id="showlab/ShowUI-2B"
      #   --version='showlab/ShowUI-2B'
      #   --dataset_dir=$$_DATA_DIR
      #   --log_base_dir=$$_SAVE_DIR
      #   --epochs=50
      #   --steps_per_epoch=100
      #   --batch_size=1
      #   --grad_accumulation_steps=2
      #   --model_max_length=8192
      #   --exp_id="showlab/ShowUI-2B"
      #   --train_ratio="1"
      #   --train_dataset="showui"
      #   --train_json="hf_train"
      #   --val_dataset="mind2web"
      #   --precision="bf16"
      #   --attn_imple="sdpa"
      #   --workers=4
      #   --lora_r=0
      #   --lora_alpha=64
      #   --min_visual_tokens=256
      #   --max_visual_tokens=1344
      #   --num_turn=100
      #   --crop_min=0.5
      #   --crop_max=1.5
      #   --record_sample
      #   --lr=0.0001
      #   --uniform_prompt
      #   --ds_zero="zero2"
      #   --gradient_checkpointing
      #   --eval_only
      #   --interleaved_history="vtvt"
      #   --num_history=1
      # zs for qwen2.5-vl-3b-instruct
      - python -m torch.distributed.run --nproc_per_node 8 --nnodes 1 --node_rank 0 --master_port 9600  --master_addr node-0 train.py
        --wandb_key=$$WANDB_API_KEY
        --model_id="Qwen/Qwen2.5-VL-3B-Instruct"
        --version='Qwen/Qwen2.5-VL-3B-Instruct'
        --dataset_dir=$$_DATA_DIR
        --log_base_dir=$$_SAVE_DIR
        --epochs=50
        --steps_per_epoch=100
        --batch_size=1
        --grad_accumulation_steps=2
        --model_max_length=8192
        --exp_id="Qwen2.5-VL-3B-Instruct-VTVT-2-ZS"
        --train_ratio="1"
        --train_dataset="showui"
        --train_json="hf_train"
        --val_dataset="mind2web"
        --precision="bf16"
        --attn_imple="sdpa"
        --workers=4
        --lora_r=0
        --lora_alpha=64
        --min_visual_tokens=256
        --max_visual_tokens=1344
        --num_turn=100
        --crop_min=0.5
        --crop_max=1.5
        --record_sample
        --lr=0.0001
        --uniform_prompt
        --ds_zero="zero2"
        --gradient_checkpointing
        --eval_only
        --interleaved_history="vtvt"
        --num_history=2
      - sleep 1d
    submit_args:
      container_args:
        # shm_size: 256g
        SHARED_MEMORY_PERCENT: 0.8
      env: # set environment variables
        _AZUREML_SINGULARITY_JOB_UAI: /subscriptions/2cd190bb-b42a-477c-b1bb-2f20932d8dc5/resourcegroups/chehao/providers/Microsoft.ManagedIdentity/userAssignedIdentities/lightleap
        # NCCL_IB_DISABLE: 0
        NCCL_DEBUG: INFO
        # NCCL_IB_TIMEOUT: 60
        # MKL_THREADING_LAYER: GNU
        WANDB_API_KEY: "75804c2ef8d16e3c694a4849f75596349142b31c"
        _DATA_DIR: "/mnt/data/datasets/"
        # _SAVE_DIR="/home/t-rsun/code/Adaptive-Multimodal-Agent/results/mind2web/ShowUI_2B_ZS"
        # _SAVE_DIR="/home/t-rsun/code/Adaptive-Multimodal-Agent/results/mind2web/ShowUI_2B_SFT" # nlp-13 -> nlp-14
        # _SAVE_DIR: "/mnt/model/results/mind2web/QwenVL_3B_ZS"
        _SAVE_DIR: "/mnt/model/results/mind2web/Long_Context"
    # tags: [Project_Name:Cost-Effective_Multimodal_Agentic_Foundation_Models_for_Digital_and_Physical_Worlds,ProjectID:PRJ-0441-A24,Experiment:Video_magma_finetuning]
  # max_trials: 36
  # type: grid
  # params:
  #   - name: nnodes
  #     spec: discrete
  #     values: [
  #       8
  #       #4
  #     ]    
  #   - name: llm_model
  #     spec: discrete
  #     values: [
  #       #magma/checkpoints/pt-Qwen/Qwen2.5-0.5B-Instruct-all-bs4-ep3-bimsz512-ncrops4-anyrescrop-seqlen4096-4e-05-cosine-0.0_openx_magma_trace_coin_ego4d_sthv2_epic_seeclick_magma_sharegpt4v_10M_iseTrue_ihTrue_tseFalse_tsdTrue_rtptsTrue_somtomTrue_qsz100-nnodes8/checkpoint-42000
  #       #magma/checkpoints_copy/data_scaling_row3_col7/checkpoint-16000
  #       #magma/checkpoints_copy/data_scaling_row3_col7/checkpoint-11000
  #       #magma/checkpoints_copy/data_scaling_row3_col3/checkpoint-51000
  #       #magma/checkpoints_copy/data_scaling_row3_col3/checkpoint-43000
  #       #magma/checkpoints_copy/data_scaling_row3_col3/checkpoint-22000
  #       #magma/checkpoints_copy/pretraining_col34/checkpoint-42000
  #       #magma/checkpoints_copy/data_scaling_row2_col3/checkpoint-28000
  #       #magma/checkpoints_copy/data_scaling_row1_col4/checkpoint-22830
  #       #magma/checkpoints_copy/finetune-none-bs4-ep3-bimsz512-ncrops4-anyrescrop-seqlen4096-1e-05-constant-0.0_coin_howto100m_ego4d_sthv2_epic_seeclick_llava_sharegpt4v_vision2ui_1M_iseTrue_ihTrue_tseFalse_tsdTrue_rtptsTrue_somtomTrue_qsz256-nnodes8-llama3-8B/checkpoint-15000
  #       #magma/checkpoints_copy/finetune-none-bs8-ep3-bimsz512-ncrops4-anyrescrop-seqlen3072-1e-5-constant-0.0_openx_magma_trace_coin_howto100m_ego4d_sthv2_epic_seeclick_llava_sharegpt4v_vision2ui_-1_iseTrue_ihTrue_tseFalse_tsdTrue_rtptsTrue_somtomTrue_qsz256-nnodes5/checkpoint-56000
  #       #magma/checkpoints/finetune-none-bs8-ep3-bimsz512-ncrops4-anyrescrop-seqlen3072-1e-5-constant-0.0_openx_magma_trace_coin_howto100m_ego4d_sthv2_epic_seeclick_llava_sharegpt4v_vision2ui_-1_iseTrue_ihTrue_tseFalse_tsdTrue_rtptsTrue_somtomTrue_qsz256-nnodes5/checkpoint-56000
  #       #magma/checkpoints/finetune-all-bs8-ep1-bimsz512-ncrops4-anyrescrop-seqlen3072-1e-5-cosine-0.0_openx_magma_trace_coin_howto100m_ego4d_sthv2_epic_seeclick_llava_sharegpt4v_vision2ui_-1_iseTrue_ihTrue_tseFalse_tsdTrue_rtptsTrue_qsz256-nnodes10-zero1
  #       #magma/checkpoints/finetune-all-bs8-ep1-bimsz512-ncrops4-anyrescrop-seqlen3072-1e-5-cosine-0.0_openx_magma_trace_coin_howto100m_ego4d_sthv2_epic_seeclick_llava_sharegpt4v_vision2ui_-1_iseTrue_ihTrue_tseFalse_tsdTrue_rtptsTrue_qsz256-nnodes10-zero1/
  #       magma/checkpoints_copy/magma-llama-3-8b-instruct-hf/
  #     ]
  #   - name: sft_data
  #     spec: discrete
  #     values: [
  #       #'llava1.5'
  #       'video_instruction_tuning_cluster'
  #     ]
  #   - name: training_size
  #     spec: discrete
  #     values: [
  #       -1
  #     ]
  #   - name: base_img_size
  #     spec: discrete
  #     values: [
  #       256
  #       #512
  #     ]
  #   - name: max_num_frames
  #     spec: discrete
  #     values: [
  #       #12
  #       #16
  #       32
  #       #48
  #       #64
  #       #96
  #       #128 
  #       #256
  #     ]
  #   - name: max_num_crops
  #     spec: discrete
  #     values: [
  #       # each crop produces 64 tokens
  #       #1
  #       4, 
  #       # 6,
  #       #4, 
  #       # 16,
  #       # 25, 
  #       # 36
  #     ]
  #   - name: img_anyres_strategy
  #     spec: discrete
  #     values: [
  #       'crop',
  #       # 'global'
  #     ]      
  #   - name: max_seq_len
  #     spec: discrete
  #     values: [
  #       # 2048,
  #       # 3072,
  #       #4096,
  #       # 5184,
  #       #6144,
  #       # 7168,
  #       #8192
  #       #12288
  #       #16384
  #       20480
  #     ]      
  #   - name: tune_vision_tokenizer
  #     spec: discrete
  #     values: [
  #       #'none',
  #       'all',
  #     ]
  #   - name: ft_per_gpu_bs
  #     spec: discrete
  #     values: [
  #       1
  #     ]
  #   - name: accum_steps
  #     spec: discrete
  #     values: [
  #       #2
  #       4
  #     ]
  #   - name: finetune_eps
  #     spec: discrete
  #     values: [
  #       1
  #       # 4,
  #       # 8,
  #     ]
  #   - name: lr
  #     spec: discrete
  #     values: [
  #       1e-5,
  #     ]
  #   - name: wdecay
  #     spec: discrete
  #     values: [
  #       0.0000,
  #     ]      
  #   - name: mm_use_trace_start_end
  #     spec: discrete
  #     values: [
  #       False, 
  #       # True
  #     ]
  #   - name: mm_use_trace_speed
  #     spec: discrete
  #     values: [
  #       False, 
  #       # True
  #     ]
  #   - name: mm_use_image_start_end
  #     spec: discrete
  #     values: [
  #       #False, 
  #       True
  #     ]      
  #   - name: remove_static_trace_pts
  #     spec: discrete
  #     values: [
  #       False
  #       # True
  #     ]
  #   - name: mm_use_spatial_pooling
  #     spec: discrete
  #     values: [
  #       False, 
  #       #True
  #     ] 
  #   - name: spatial_quant_size
  #     spec: discrete
  #     values: [
  #       # 512,
  #       256, 
  #       # 128
  #     ]
  #   - name: lr_scheduler
  #     spec: discrete
  #     values: [
  #       'cosine',
  #       # 'cosine_with_min_lr', 
  #       # 'constant'
  #     ]